{"cells":[{"cell_type":"markdown","id":"32b1dc5e","metadata":{"id":"32b1dc5e"},"source":["### This file extracts audio files from directories.\n","1. The audio files are stored in different folders with their respective class name.\n","2. The functions will extract melspectrogram and mfcc from each audio files\n","3. For alignment, the audio files are sliced according the `seg size`(in this sample,seg=30).\n","4. If the segmentation has sample size less than `seg size` but more than half of it, it will be padded with 0s. Else erased."]},{"cell_type":"code","execution_count":1,"id":"8bd8ac4a","metadata":{"executionInfo":{"elapsed":346,"status":"ok","timestamp":1656390435210,"user":{"displayName":"XiangRui Liu","userId":"12428559882116548779"},"user_tz":-480},"id":"8bd8ac4a"},"outputs":[],"source":["import os\n","import numpy as np\n","import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":2,"id":"5c3ea234","metadata":{},"outputs":[{"data":{"text/plain":["'1.21.5'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["np.__version__"]},{"cell_type":"code","execution_count":3,"id":"6344af79","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1656390438195,"user":{"displayName":"XiangRui Liu","userId":"12428559882116548779"},"user_tz":-480},"id":"6344af79"},"outputs":[],"source":["root_dir = \"/home/assa8945/MMC/Dataset/Watkins_full\"\n","# root_dir = \"D:\\A\\Marine Mammal Detection\\datasets\\AAAI23\\\\3classes\\\\audio\"\n","seg = 240\n","sample_rate = 4000\n","# save_dir =  \"D:\\A\\Marine Mammal Detection\\datasets\\\\\"+\"100khz_seg_\"+str(seg)+\"_mean_padded\"\n","save_dir = \"/home/assa8945/MMC/Dataset/features/watkins_full/full_features\"\n","if os.path.exists(save_dir):\n","    pass\n","else:\n","    os.mkdir(save_dir)"]},{"cell_type":"code","execution_count":4,"id":"6a01ff99","metadata":{},"outputs":[],"source":["label_name={}"]},{"cell_type":"code","execution_count":5,"id":"71dec5d9","metadata":{},"outputs":[{"data":{"text/plain":["{0: 'Juan Fernandez Fur Seal',\n"," 1: 'sperm whale',\n"," 2: 'Pantropical Spotted Dolphin',\n"," 3: 'Finback Whale',\n"," 4: 'Humpback Whale',\n"," 5: 'Common Dolphin',\n"," 6: 'Long-Finned Pilot Whale',\n"," 7: 'Weddell Seal',\n"," 8: 'Ross Seal',\n"," 9: 'Gray Seal',\n"," 10: 'Steller Sea Lion',\n"," 11: 'Harp Seal',\n"," 12: 'Finless Porpoise',\n"," 13: 'Harbor Porpoise',\n"," 14: 'West Indian Manatee',\n"," 15: 'Southern Right Whale',\n"," 16: 'Bottlenose Dolphin',\n"," 17: 'New Zealand Fur Seal',\n"," 18: 'Dusky Dolphin',\n"," 19: 'Rough-Toothed Dolphin',\n"," 20: 'Ribbon Seal',\n"," 21: 'Short-Finned (Pacific) Pilot Whale',\n"," 22: 'Ringed Seal',\n"," 23: 'Tucuxi Dolphin',\n"," 24: 'Spinner Dolphin',\n"," 25: 'Leopard Seal',\n"," 26: 'Hooded Seal',\n"," 27: 'Northern Right Whale',\n"," 28: \"Fraser's Dolphin\",\n"," 29: \"Grampus, Risso's Dolphin\",\n"," 30: 'Blue Whale',\n"," 31: 'Sea Otter',\n"," 32: 'False Killer Whale',\n"," 33: 'Striped Dolphin',\n"," 34: 'White-sided Dolphin',\n"," 35: 'Irawaddy Dolphin',\n"," 36: 'Narwhal',\n"," 37: 'White-beaked Dolphin',\n"," 38: \"Dall's Porpoise\",\n"," 39: 'Walrus',\n"," 40: 'Killer Whale',\n"," 41: 'Minke Whale',\n"," 42: 'Harbour Seal',\n"," 43: 'Gray Whale',\n"," 44: 'Clymene Dolphin',\n"," 45: 'Spotted Seal',\n"," 46: 'Boutu, Amazon River Dolphin',\n"," 47: 'Bowhead Whale',\n"," 48: \"Heaviside's Dolphin\",\n"," 49: 'Long Beaked (Pacific) Common Dolphin',\n"," 50: 'Beluga, White Whale',\n"," 51: 'Atlantic Spotted Dolphin',\n"," 52: 'Melon Headed Whale',\n"," 53: 'Bearded Seal'}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["for i,value in enumerate(os.listdir(root_dir)):\n","    label_name[i]=value\n","label_name"]},{"cell_type":"code","execution_count":6,"id":"ee697d62","metadata":{},"outputs":[],"source":["# keeplist = [1.0, 3.0, 4.0, 40.0, 21.0, 6.0, 2.0, 24.0, 5.0, 16.0, 7.0, 47.0, 33.0]"]},{"cell_type":"code","execution_count":null,"id":"d6dd1115","metadata":{},"outputs":[],"source":["keeplist = ['sperm whale','Finback Whale','Humpback Whale','Killer Whale','Short-Finned (Pacific) Pilot Whale','Long-Finned Pilot Whale','Pantropical Spotted Dolphin','Spinner Dolphin','Common Dolphin','Bottlenose Dolphin','Weddell Seal','Bowhead Whale','Striped Dolphin']"]},{"cell_type":"code","execution_count":7,"id":"bb34d414","metadata":{},"outputs":[{"data":{"text/plain":["13"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# len(keeplist)"]},{"cell_type":"code","execution_count":8,"id":"a2e50c3f","metadata":{},"outputs":[],"source":["# import csv\n","# class_limited = []\n","# for i in keeplist:\n","#     class_limited.extend([label_name[i]])\n","# len(class_limited)"]},{"cell_type":"code","execution_count":10,"id":"f489d1fb","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1656390438196,"user":{"displayName":"XiangRui Liu","userId":"12428559882116548779"},"user_tz":-480},"id":"f489d1fb"},"outputs":[],"source":["def slicing_and_padding_data_list(data_list,seg=30):\n","    \"\"\"_summary_\n","\n","    Args:\n","        data_list (_type_): A data list that contains 2D arrays data.\n","        seg (int, optional): The length of each sliced data. Defaults to 30.\n","    \"\"\"\n","    new_data_list = []\n","    for data in data_list: # loop through each data in the data list \n","        data_length = data.shape[0] # The length of original 2D data sample.\n","        seg_counter = 0\n","        while data_length > 0:\n","            if data_length>=seg:\n","\n","                new_data_list.append(data[seg_counter:seg_counter + seg,:])\n","                data_length -= seg\n","                seg_counter += seg\n","\n","            elif data_length >= seg/2:  #when the left data_length is less than seg size but large than half of seg size, pad it up to seg size\n","                pad = np.zeros((seg-data_length,data.shape[1]))\n","                padded_data = np.vstack((data[seg_counter:,:],pad))\n","                # print(padded_data.shape)\n","                new_data_list.append(padded_data)\n","                break\n","            else: #when the row number is less than half of the segment , pass the rows.\n","                break\n","    return new_data_list\n"]},{"cell_type":"code","execution_count":11,"id":"46a7e23d","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1656390438196,"user":{"displayName":"XiangRui Liu","userId":"12428559882116548779"},"user_tz":-480},"id":"46a7e23d"},"outputs":[],"source":["def slicing_and_padding_single_data(data,seg=30):\n","    \"\"\"_summary_\n","\n","    Args:\n","        data_list (_type_): A data list that contains 2D arrays data.\n","        seg (int, optional): The length of each sliced data. Defaults to 30.\n","    \"\"\"\n","    new_data_list = []\n","\n","    data_length = data.shape[0] # The length of original 2D data sample.\n","    seg_counter = 0\n","    while data_length > 0:\n","        if data_length>=seg:\n","\n","            new_data_list.append(data[seg_counter:seg_counter + seg,:])\n","            data_length -= seg\n","            seg_counter += seg\n","\n","        elif data_length >= seg/2:  #when the left data_length is less than seg size but large than half of seg size, pad it up to seg size\n","            \n","           # pad the sample mean\n","            average = data.mean(axis=0)\n","            missing_length = seg-data_length\n","            averages_pads = np.tile(average,(missing_length,1))\n","            padded_data = np.vstack((data[seg_counter:,:],averages_pads))\n","            # print(padded_data.shape)\n","\n","            #pad 0s \n","            # pad = np.zeros((seg-data_length,data.shape[1]))\n","            # padded_data = np.vstack((data[seg_counter:,:],pad))\n","\n","\n","            #pad last frame\n","            # last_frame = data[-1,:]\n","            # missing_length = seg-data_length\n","            # pad = np.tile(last_frame,(missing_length,1))\n","            # padded_data = np.vstack((data[seg_counter:,:],pad))\n","\n","\n","\n","            # print(padded_data.shape)\n","            new_data_list.append(padded_data)\n","            break\n","        else: #when the row number is less than half of the segment , pass the rows.\n","            break\n","    return new_data_list\n"]},{"cell_type":"code","execution_count":12,"id":"7a3bed76","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1082467,"status":"ok","timestamp":1656391520657,"user":{"displayName":"XiangRui Liu","userId":"12428559882116548779"},"user_tz":-480},"id":"7a3bed76","outputId":"b14c76b8-fb83-4bfd-8879-00407ccf4a9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["['sperm whale', 'Pantropical Spotted Dolphin', 'Finback Whale', 'Humpback Whale', 'Common Dolphin', 'Long-Finned Pilot Whale', 'Weddell Seal', 'Bottlenose Dolphin', 'Short-Finned (Pacific) Pilot Whale', 'Spinner Dolphin', 'Striped Dolphin', 'Killer Whale', 'Bowhead Whale']\n","going through folder /home/assa8945/MMC/Dataset/Watkins_full/sperm whale\n"]},{"name":"stderr","output_type":"stream","text":["/home/assa8945/.local/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n","  return f(*args, **kwargs)\n"]}],"source":["def batch_feature_extraction(dir,sample_rate,seg=seg,):\n","    \"\"\"_summary_\n","\n","    Args:\n","        dir (_type_): Root folder directory that contains differents classes' folders.\n","\n","    Returns:\n","        _type_: spectrogram list, mfcc list\n","    \"\"\"\n"," \n","    # Save the folders' directories to a list according to the classes.\n","    \n","    folder_list = []\n","    class_list = []\n","    for root,folders,files in os.walk(dir):\n","        for folder in folders:\n","            # if folder not in class_limited:\n","            #     continue\n","            folder_list.append(os.path.join(root,folder))\n","            class_list.append(folder)\n","    # print(folder_list)\n","\n","    print(class_list)\n","    for folder_dir in folder_list:\n","        \n","        specs = []\n","        mfccs = []\n","        print('going through folder',folder_dir)\n","        for file in os.listdir(folder_dir):\n","            \n","            if file.endswith(\".csv\"):\n","                continue\n","\n","            audio_name = file\n","            audio_dir = os.path.join(folder_dir,audio_name)\n","            # print(audio_dir)\n","\n","            y, sr = librosa.load(audio_dir,sr=sample_rate) #default sample rate 22050\n","            if len(y)<2048:\n","                print('The length of audio is less than 2048 samples ',audio_dir)\n","                continue\n","            \n","            # Calculate spectrogram\n","            spec = librosa.feature.melspectrogram(y=y,sr=sr,n_mels=240)\n","            spec = librosa.power_to_db(spec)\n","            spec = spec.T\n","            # print(spec.shape)\n","            modified_spec = slicing_and_padding_single_data(spec,seg=seg)\n","            specs.extend(modified_spec)           \n","            \n","            # Calculate mfcc\n","            mfcc = librosa.feature.mfcc(y=y,sr=sr,n_mfcc=40)\n","            mfcc = mfcc.T\n","            # print(mfcc.shape)\n","            modified_mfcc = slicing_and_padding_single_data(mfcc,seg=seg)\n","            \n","            mfccs.extend(modified_mfcc)\n","\n","#-------------------------------------------------------------------------------------------\n","# Assert that all the data has correct shape\n","        assert len(specs)==len(mfccs),AssertionError\n","        print('There are {} sliced and padded data samples under \\n{}\\n'.format(len(specs),folder_dir))\n","        \n","        for i in specs:\n","            assert i.shape[0]==seg,print(i.shape)\n","        for j in mfccs:\n","            assert j.shape[0]==seg,print(j.shape)\n","#-------------------------------------------------------------------------------------------\n","\n","\n","#-------------------------------------------------------------------------------------------\n","# Save the data into csv files to respective directories\n","        specs = np.array(specs)\n","        # print(specs.shape)\n","        mfccs = np.array(mfccs)\n","        # print(mfccs.shape)\n","        reshaped_specs = specs.reshape(specs.shape[0],-1)\n","        reshaped_mfccs = mfccs.reshape(mfccs.shape[0],-1)\n","\n","        np.savetxt(os.path.join(folder_dir,'mfccs.csv'), reshaped_mfccs, delimiter=\",\")\n","        np.savetxt(os.path.join(folder_dir,'specs.csv'), reshaped_specs, delimiter=\",\")\n","\n","\n","batch_feature_extraction(root_dir,sample_rate=sample_rate,seg=seg)"]},{"cell_type":"code","execution_count":null,"id":"7a08cef9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142850,"status":"ok","timestamp":1656392823219,"user":{"displayName":"XiangRui Liu","userId":"12428559882116548779"},"user_tz":-480},"id":"7a08cef9","outputId":"e0c3d792-5c58-4fec-d50a-a6ef1879da3e"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/assa8945/MMC/codes/data_prep/Dataset_prep(Watkins).ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bokas-cdlr104340.ws.ok.ubc.ca/home/assa8945/MMC/codes/data_prep/Dataset_prep%28Watkins%29.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mThe labels have shape of \u001b[39m\u001b[39m'\u001b[39m,collected_y\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bokas-cdlr104340.ws.ok.ubc.ca/home/assa8945/MMC/codes/data_prep/Dataset_prep%28Watkins%29.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m collected_specs,collected_mfccs,collected_y\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bokas-cdlr104340.ws.ok.ubc.ca/home/assa8945/MMC/codes/data_prep/Dataset_prep%28Watkins%29.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m collected_specs,collected_mfccs,collected_y \u001b[39m=\u001b[39m merge_and_labels(root_dir,seg)\n","\u001b[1;32m/home/assa8945/MMC/codes/data_prep/Dataset_prep(Watkins).ipynb Cell 14\u001b[0m in \u001b[0;36mmerge_and_labels\u001b[0;34m(root_dir, seg)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bokas-cdlr104340.ws.ok.ubc.ca/home/assa8945/MMC/codes/data_prep/Dataset_prep%28Watkins%29.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bokas-cdlr104340.ws.ok.ubc.ca/home/assa8945/MMC/codes/data_prep/Dataset_prep%28Watkins%29.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m folder_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root_dir,folder)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bokas-cdlr104340.ws.ok.ubc.ca/home/assa8945/MMC/codes/data_prep/Dataset_prep%28Watkins%29.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m specs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mloadtxt(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(folder_dir,\u001b[39m\"\u001b[39;49m\u001b[39mspecs.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m), delimiter\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bokas-cdlr104340.ws.ok.ubc.ca/home/assa8945/MMC/codes/data_prep/Dataset_prep%28Watkins%29.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m mfccs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mloadtxt(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_dir,\u001b[39m\"\u001b[39m\u001b[39mmfccs.csv\u001b[39m\u001b[39m\"\u001b[39m), delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bokas-cdlr104340.ws.ok.ubc.ca/home/assa8945/MMC/codes/data_prep/Dataset_prep%28Watkins%29.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Restore data to 3D\u001b[39;00m\n","File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py:1148\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[39m# read data in chunks and fill it into an array via resize\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[39m# over-allocating and shrinking the array later may be faster but is\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[39m# probably not relevant compared to the cost of actually reading and\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[39m# converting the data\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1148\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m read_data(_loadtxt_chunksize):\n\u001b[1;32m   1149\u001b[0m     \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m         X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(x, dtype)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py:999\u001b[0m, in \u001b[0;36mloadtxt.<locals>.read_data\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWrong number of columns at line \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    996\u001b[0m                      \u001b[39m%\u001b[39m line_num)\n\u001b[1;32m    998\u001b[0m \u001b[39m# Convert each value according to its column and store\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m items \u001b[39m=\u001b[39m [conv(val) \u001b[39mfor\u001b[39;00m (conv, val) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(converters, vals)]\n\u001b[1;32m   1001\u001b[0m \u001b[39m# Then pack it according to the dtype's nesting\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m items \u001b[39m=\u001b[39m pack_items(items, packing)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py:999\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWrong number of columns at line \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    996\u001b[0m                      \u001b[39m%\u001b[39m line_num)\n\u001b[1;32m    998\u001b[0m \u001b[39m# Convert each value according to its column and store\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m items \u001b[39m=\u001b[39m [conv(val) \u001b[39mfor\u001b[39;00m (conv, val) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(converters, vals)]\n\u001b[1;32m   1001\u001b[0m \u001b[39m# Then pack it according to the dtype's nesting\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m items \u001b[39m=\u001b[39m pack_items(items, packing)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py:736\u001b[0m, in \u001b[0;36m_getconv.<locals>.floatconv\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m0x\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m x:\n\u001b[1;32m    735\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mfloat\u001b[39m\u001b[39m.\u001b[39mfromhex(x)\n\u001b[0;32m--> 736\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mfloat\u001b[39;49m(x)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["def merge_and_labels(root_dir,seg):\n","    folders = os.listdir(root_dir)\n","    labels = []\n","    counter = 0\n","    collected_specs = np.empty((1,seg,240))\n","    collected_mfccs = np.empty((1,seg,40))\n","    collected_y = np.empty((1,1))\n","    for folder in folders:\n","        if folder.endswith('csv'):\n","            continue \n","        # if folder not in class_limited:\n","        #     continue\n","        folder_dir = os.path.join(root_dir,folder)\n","        \n","        specs = np.loadtxt(os.path.join(folder_dir,\"specs.csv\"), delimiter=\",\")\n","        mfccs = np.loadtxt(os.path.join(folder_dir,\"mfccs.csv\"), delimiter=\",\")\n","\n","        # Restore data to 3D\n","        specs = specs.reshape(-1,seg,240)\n","        mfccs = mfccs.reshape(-1,seg,40)\n","        y = np.full((specs.shape[0],1),counter)\n","\n","        # Stack them up\n","        collected_specs = np.vstack((collected_specs,specs))\n","        collected_mfccs = np.vstack((collected_mfccs,mfccs))\n","        collected_y = np.vstack((collected_y,y))\n","        \n","        labels.append((folder,counter))\n","\n","        counter += 1\n","\n","    # Remove the empty sample 0 (created for initialization)    \n","    collected_specs = np.delete(collected_specs,0,0)\n","    collected_mfccs = np.delete(collected_mfccs,0,0)\n","    collected_y = np.delete(collected_y,0,0)\n","\n","\n","    print(labels) \n","    \n","    print('\\nThe overall mfcc data has shape of ',collected_mfccs.shape)\n","\n","    print('\\nThe overall spectrogram data has shape of ',collected_specs.shape)\n","    print('\\nThe labels have shape of ',collected_y.shape)\n","\n","    return collected_specs,collected_mfccs,collected_y\n","\n","collected_specs,collected_mfccs,collected_y = merge_and_labels(root_dir,seg)"]},{"cell_type":"code","execution_count":null,"id":"27e6f05a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":152073,"status":"ok","timestamp":1656393049635,"user":{"displayName":"XiangRui Liu","userId":"12428559882116548779"},"user_tz":-480},"id":"27e6f05a","outputId":"1839da57-2fc2-4fce-dca3-1608e71ceac7"},"outputs":[{"name":"stdout","output_type":"stream","text":["(330738, 60, 40)\n","saving data to:  /home/assa8945/MMC/Dataset/features/watkins_full/xx_classes_300limited\n","completed\n"]}],"source":["# The 3D arrays are reshaped into 2D arrays so can be save to csv files\n","reshaped_mfccs=collected_mfccs.reshape(collected_mfccs.shape[0],-1)\n","reshaped_specs=collected_specs.reshape(collected_specs.shape[0],-1)\n","print(collected_mfccs.shape)\n","\n","if os.path.exists(save_dir):\n","    pass\n","else:\n","    os.mkdir(save_dir)\n","\n","print(\"saving data to: \",save_dir)\n","# np.savetxt(os.path.join(save_dir,'Full_mfccs.csv'), reshaped_mfccs, delimiter=\",\")\n","# np.savetxt(os.path.join(save_dir,'Full_specs.csv'), reshaped_specs, delimitmmer=\",\")\n","np.savez(os.path.join(save_dir,'mfccs.npy.npz'),*collected_mfccs)\n","np.savez(os.path.join(save_dir,'specs.npy.npz'),*collected_specs)\n","np.savetxt(os.path.join(save_dir,'labels.csv'), collected_y, delimiter=\",\")\n","\n","print('completed')"]}],"metadata":{"colab":{"name":"Dataset_prep(Watkins).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"}}},"nbformat":4,"nbformat_minor":5}
